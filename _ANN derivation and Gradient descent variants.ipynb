{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13da0f28",
   "metadata": {},
   "source": [
    "# Deep Learning : Forward/backward and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09631c",
   "metadata": {},
   "source": [
    "In this notebook I will implement:\n",
    "Simple forward and backward pass on a custom made numpy ANN based on my the derivations in the pdf [here](https://github.com/YoelGraumann/DeepLearning-Simple-Implementation/blob/main/Forward%20and%20Backward%20pass%20ANN.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c0685",
   "metadata": {},
   "source": [
    "And the gradient descent algorithm which includes\n",
    "Vanilla GD, SGD, GD with momentum, GD with exponential decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148adc27",
   "metadata": {},
   "source": [
    "### Forward and backward pass implementation, based my derivations in pdf i posted on github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966b159",
   "metadata": {},
   "source": [
    "Each fully connected layer object must be initialized with:\n",
    "* neurons_before : the number of neurons or inputs from previous layer. -integer\n",
    "* current_neurons: the number of neurons in this layer. - integer\n",
    "* weights: =1 initializes the weights_matrix to be a matrix of 1s, by default they will be initialized by the standard normal distribution\n",
    "* bias: =1 initializes the bias_vector to be a vector of 1s, =0 initializes the bias vector to be of 0s,by default they will be initialized by the standard normal distribution.\n",
    "\n",
    "Note that the bias_vecor will a row vector of 1 by the number of the current neurons\n",
    "note that the weights_matrix is a matrix where the columns correspond to the neurons in the current layer. In other words:\n",
    "the weights in column 1, will be the weights used for the input of neuron 1 in the hidden layer, and so on.\n",
    "\n",
    "the forward_step will do a forward step through the fully connected layer, saving x for layer \n",
    "the forward_step also calculates z=X*W+B\n",
    "\n",
    "the backward_step will do a backward step through the layer. It will calculate the grad w.r.t\n",
    "weights biases and the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4518d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "class Fully_Connected_Layer:  \n",
    "    ## initialization:\n",
    "    ### the weights=10 and bias=10 will ensure that if there is no input, they will be initialized by the standard normal dist\n",
    "    def __init__(self,neurons_before,current_neurons,weights=10,bias=10):\n",
    "        ## default initializations\n",
    "        self.weights_matrix=np.random.standard_normal((neurons_before,current_neurons))\n",
    "        self.bias_vector=np.random.standard_normal((1,current_neurons))\n",
    "        # scanning for wanted weights and biases\n",
    "        'Note that it does not make any sense to initialize the' \n",
    "        'weights_matrix to be of 0s since it will kill the Neural Network'\n",
    "        if weights==1:\n",
    "            self.weights_matrix=np.ones((neurons_before,current_neurons))\n",
    "        if bias==1:\n",
    "            self.bias_vector=np.ones((1,current_neurons))\n",
    "        if bias==0:\n",
    "            self.bias_vector=np.zeros((1,current_neurons))                                   \n",
    "    ## forward pass                                   \n",
    "    def forward_step(self,x):\n",
    "        ## Saving the original input values for the forward step for later in self.x\n",
    "        self.x=x\n",
    "        ### calculating z which is , which is X*W+B (no need to transpose the weights_matrix since its created accordingly)\n",
    "        self.z=np.dot(x,self.weights_matrix)+self.bias_vector\n",
    "        \n",
    "        \n",
    "    def backward_step(self,grad_vals,use_matrix_form=0):\n",
    "        ## gradients on parameters\n",
    "        '''\n",
    "        Note that use_matrix_form is for the edge case when backpropagating to the first layer of the NN\n",
    "        gives a bunch of shape errors. So when doing a backward step for the first layer of the NN\n",
    "        do: use_matrix_form=1\n",
    "        '''\n",
    "        if use_matrix_form==0:\n",
    "            self.grad_inputs=np.dot(grad_vals,self.weights_matrix.T)\n",
    "            self.grad_biases=np.sum(grad_vals,axis=0,keepdims=True)\n",
    "            self.grad_weights=np.dot(self.x.T,grad_vals)\n",
    "        else:\n",
    "            self.grad_inputs=np.dot(grad_vals,self.weights_matrix.T)\n",
    "            self.grad_biases=np.sum(grad_vals,axis=0,keepdims=True)\n",
    "            self.grad_weights=np.matrix(self.x).T*np.matrix(grad_vals)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5394e6",
   "metadata": {},
   "source": [
    "Note: The derivative of the sigmoid function is :\n",
    "$$\\sigma(x)(1-\\sigma(x))$$\n",
    "I'll use that fact for the backward_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d253f",
   "metadata": {},
   "source": [
    "Now I will create classes for Relu, sigmoid and identity.\n",
    "Their forward pass will take in an input x  and save them for later, then output the y\n",
    "Their backward pass will calculate the derivatives and will be able to send them to the next layer in the backprop algo.\n",
    "All the Activation Layers will be in the following code block:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58d94366",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identity activation layer which was used to understand the layers, worth keeping in.\n",
    "\n",
    "class Identity_Activation_Layer:\n",
    "    def forward_step(self,x):\n",
    "        self.x=x\n",
    "        self.y=x\n",
    "    def backward_step(self,grad_vals):\n",
    "        self.grad_inputs=grad_vals.copy()\n",
    "        \n",
    "### ReLU activation Layer\n",
    "class ReLU_Activation_Layer:\n",
    "    '''\n",
    "    ReLU's output is the maximum of the (0,x)\n",
    "    it will backprop the gradvalus from before if they are non-zero positive,\n",
    "    else it will backprop 0.\n",
    "    '''\n",
    "    ### forward step definition\n",
    "    def forward_step(self,x):\n",
    "        ## the input is x\n",
    "        self.x=x\n",
    "        ### The output is y.\n",
    "        self.y=np.maximum(0,x)\n",
    "    ### backwardward step:\n",
    "    def backward_step(self,grad_vals):\n",
    "        #saving to grad_inputs so as to not change the original grad_vals input coming in during backprop\n",
    "        self.grad_inputs=grad_vals.copy()\n",
    "        self.grad_inputs[self.grad_inputs<=0]=0\n",
    "       \n",
    "    \n",
    "### Sigmoid activation layer\n",
    "class Sigmoid_Activation_Layer:\n",
    "    ## forwardstep\n",
    "    def forward_step(self,x):\n",
    "        ## as always, saving the x's for later.\n",
    "        self.x=x\n",
    "        #y is the output, which is equal to the sigmoid.\n",
    "        self.y=1/(1+np.exp(-x))\n",
    "    def backward_step(self,grad_vals):\n",
    "        ### derivative of the sigmoid, and by applying the chain rule we use the prev grad_vals\n",
    "        self.grad_inputs=self.y*(1-self.y)*grad_vals\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce2c88f",
   "metadata": {},
   "source": [
    "The final ingerdient in our code is to create the RSS loss function. This function will calculate the loss on its forward_step, and calculate the derivative on its backward_step, starting the chain of events of back propagation.\n",
    "Remember that the RSS is given by:\n",
    "\n",
    "\n",
    "$$RSS=\\sum_{i=1}^n (Y_i-\\hat{Y_i})^2$$\n",
    "\n",
    "Where $$\\hat{Y_i}$$ is the is the final hidden layer's prediction, and $$Y_i$$ is the real value\n",
    "\n",
    "Hence I will use this formula for backpropagation:\n",
    "\n",
    "$$-2(Y_i-\\hat{Y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdb3f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSS_Loss:\n",
    "    #simply calculating the loss.\n",
    "    def forward_step(self,y_real,y_hat):\n",
    "        #exactly as we defined above\n",
    "        self.rss_loss=np.sum((y_real-y_hat)**2,axis=-1)\n",
    "    # backproping\n",
    "    def backward_step(self,y_real,y_hat):\n",
    "        #exactly as we defined above.\n",
    "        self.grad_vals=-2*(y_real-y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb456c4",
   "metadata": {},
   "source": [
    "##### Using the above classes / objects, I will create the ANN network I derived in the pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002aaab",
   "metadata": {},
   "source": [
    "###### Creating the model:\n",
    "The model has an input of X=[1,2,-1] and an output of Y=0.\n",
    "Two hidden layers, where both contain 2 neurons each. The activation of these hidden layers is ReLU.\n",
    "the output layer with a single neuron with the identity as it's activation.\n",
    "All weights of the networks are initialized to 1.\n",
    "* Note that I assumed a bias of 0 for all neurons when doing the backprop calculations in the pdf. Will assume the same here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac2dbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "X=[1,2,-1]\n",
    "Y=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3137d1",
   "metadata": {},
   "source": [
    "###### Declaring the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3483bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the first layer gets 3 inputs, it has 2 neurons, the weights are all 1 and the biases are all 0\n",
    "Hidden_Layer_1=Fully_Connected_Layer(neurons_before=3,current_neurons=2,weights=1,bias=0)\n",
    "## hidden layer 1 goes through a relu activation.\n",
    "\n",
    "Activation_of_Layer_1=ReLU_Activation_Layer()\n",
    "\n",
    "# look at the drawing in the pdf to understand further. same logic as before though:\n",
    "Hidden_Layer_2=Fully_Connected_Layer(neurons_before=2,current_neurons=2,weights=1,bias=0)\n",
    "\n",
    "Activation_of_Layer_2=ReLU_Activation_Layer()\n",
    "\n",
    "### in question 1 we called the final output layer H 3, so we will do the same here\n",
    "Hidden_Layer_3=Fully_Connected_Layer(neurons_before=2,current_neurons=1,weights=1,bias=0)\n",
    "\n",
    "\n",
    "# The hidden layer 3 has the identity as the activation...\n",
    "\n",
    "Activation_of_Layer_3=Identity_Activation_Layer()\n",
    "\n",
    "## We learned during Lecture 1 that it is useful to see the loss function as a layer. we will do that here:\n",
    "\n",
    "Loss_Layer=RSS_Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47df0f",
   "metadata": {},
   "source": [
    "###### FEED FORWARD!\n",
    "Printing the output for every step so you can see we got the same values both on paper and in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93196f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "Hidden_Layer_1.forward_step(X)\n",
    "print(Hidden_Layer_1.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8bd9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "Activation_of_Layer_1.forward_step(Hidden_Layer_1.z)\n",
    "print(Activation_of_Layer_1.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26398a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "Hidden_Layer_2.forward_step(Activation_of_Layer_1.y)\n",
    "print(Hidden_Layer_2.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17492aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "Activation_of_Layer_2.forward_step(Hidden_Layer_2.z)\n",
    "print(Activation_of_Layer_2.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "300cbc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.]]\n"
     ]
    }
   ],
   "source": [
    "Hidden_Layer_3.forward_step(Activation_of_Layer_2.y)\n",
    "print(Hidden_Layer_3.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3e84ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.]]\n"
     ]
    }
   ],
   "source": [
    "Activation_of_Layer_3.forward_step(Hidden_Layer_3.z)\n",
    "print(Activation_of_Layer_3.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb08bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64.]\n"
     ]
    }
   ],
   "source": [
    "Loss_Layer.forward_step(Y,Activation_of_Layer_3.y)\n",
    "print(Loss_Layer.rss_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba351c6",
   "metadata": {},
   "source": [
    "We can see that our Forward Pass code yields the same numbers as we have calculated in Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623a8ab",
   "metadata": {},
   "source": [
    "###### BACKPROPAGATION!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa7cd358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the loss w.r.t H3 is  [[16.]]\n"
     ]
    }
   ],
   "source": [
    "Loss_Layer.backward_step(Y,Activation_of_Layer_3.y)\n",
    "print('The derivative of the loss w.r.t H3 is ',Loss_Layer.grad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12413e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.]]\n"
     ]
    }
   ],
   "source": [
    "Activation_of_Layer_3.backward_step(Loss_Layer.grad_vals)\n",
    "print(Activation_of_Layer_3.grad_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b1c9712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 16.]]\n"
     ]
    }
   ],
   "source": [
    "Hidden_Layer_3.backward_step(Activation_of_Layer_3.grad_inputs)\n",
    "print(Hidden_Layer_3.grad_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18348d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 16.]]\n"
     ]
    }
   ],
   "source": [
    "Activation_of_Layer_2.backward_step(Hidden_Layer_3.grad_inputs)\n",
    "print(Activation_of_Layer_2.grad_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "708a0d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32. 32.]]\n"
     ]
    }
   ],
   "source": [
    "Hidden_Layer_2.backward_step(Activation_of_Layer_2.grad_inputs)\n",
    "print(Hidden_Layer_2.grad_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07e4e1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32. 32.]]\n"
     ]
    }
   ],
   "source": [
    "Activation_of_Layer_1.backward_step(Hidden_Layer_2.grad_inputs)\n",
    "print(Activation_of_Layer_1.grad_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "205091ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hidden_Layer_1.backward_step(Activation_of_Layer_1.grad_inputs,use_matrix_form=1)\n",
    "## Done backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6021d",
   "metadata": {},
   "source": [
    "Let's print the derivatives w.r.t. weights and biases  for $$H^{(3)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e8e82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[64.],\n",
       "        [64.]]),\n",
       " array([[16.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden_Layer_3.grad_weights , Hidden_Layer_3.grad_biases ## Same output as the derivations by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb1137",
   "metadata": {},
   "source": [
    "Let's print the derivatives w.r.t. weights and biases  for $$H^{(2)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "850fb19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32., 32.],\n",
       "       [32., 32.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden_Layer_2.grad_weights ## Same output as the derivations by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c02349e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 16.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden_Layer_2.grad_biases ### Same output as the derivations by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce7e42",
   "metadata": {},
   "source": [
    "Let's print the derivatives w.r.t. weights and biases  for $$H^{(1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c33bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 32.,  32.],\n",
       "        [ 64.,  64.],\n",
       "        [-32., -32.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden_Layer_1.grad_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73e95b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32., 32.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden_Layer_1.grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd81d3e",
   "metadata": {},
   "source": [
    "Exactly the same values as derived by hand.\n",
    "$$\\tag*{$\\blacksquare$}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840fdb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attempting batch normalization.\n",
    "\n",
    "\n",
    "class Batch_Normalization_Layer:\n",
    "    def forward_pass(self,x):\n",
    "        self.x=x\n",
    "        self.beta=np.random.standard_normal()\n",
    "        self.gamma=np.random.standard_normal()\n",
    "        ### adding epsilon so we dont accidentally divide by zero.\n",
    "        self.epsilon=0.001\n",
    "        ## gamma and beta used to scale and shift. \n",
    "        ## if gamma is learned to be the std and beta to be the mean, the batch norm layer will be\n",
    "        ### the idendity activation!\n",
    "        self.y=(x-np.mean(x))/np.sqrt((np.var(x)+self.epsilon))*self.gamma+self.beta\n",
    "    def backward_pass(self,grad_vals):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f828f6",
   "metadata": {},
   "source": [
    "### Trying out Gradient descent.\n",
    "To make it easier, I'll use some simple data generation process.\n",
    "The point is to implement gradient descent, in order to understand the workhorse behind Neural Networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fcbae",
   "metadata": {},
   "source": [
    "Some notation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ebe9f",
   "metadata": {},
   "source": [
    "Data Generation Process:\n",
    "* Let $$x_1, x_2 , x_3, x_4$$ be vectors of length 10000 with values in [0,1] each.\n",
    "* Let the noise $$\\epsilon_i$$ be i.i.d Gaussians with $$\\mu=0,\\sigma^2=1$$\n",
    "* Let the the real Y value be defined as $$Y_{real}=x_1-2x_2,+3x_3-4x_4+\\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78f8633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Generation Process\n",
    "np.random.seed(123)\n",
    "x_1=np.random.uniform(low=0,high=1,size=10000)\n",
    "x_2=np.random.uniform(low=0,high=1,size=10000)\n",
    "x_3=np.random.uniform(low=0,high=1,size=10000)\n",
    "x_4=np.random.uniform(low=0,high=1,size=10000)\n",
    "epsilon=np.random.standard_normal(size=10000)\n",
    "Y_real=x_1-2*x_2+3*x_3-4*x_4+epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5cd01",
   "metadata": {},
   "source": [
    "Let our prediction be $$Y_{pred}=\\sum_{i=0}^nw_i*x_i$$ \n",
    "where $$x_0$$ is a dummy vector of 1's.\n",
    "and $$w_i$$ are our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76ec416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0=np.ones(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733a452",
   "metadata": {},
   "source": [
    "In order to implement Gradient Descent, we want to minimize the cost function of the RSS (note we added a multiplication of 1/2 to make the derivative nicer)\n",
    "$$J(w)=RSS=\\frac12 \\sum_{i=1}^n (Y_i-\\hat{Y_i})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8186ed3",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dw}J(w)=\\sum_{i=1}^n (Y_i-\\hat{Y_i})x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b2b86",
   "metadata": {},
   "source": [
    "Each column in X_matrix represents different x values.\n",
    "a row in the X_matrix is one observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff1c5e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.69646919, 0.66314239, 0.08647754, 0.89401513],\n",
       "       [1.        , 0.28613933, 0.8397864 , 0.58416443, 0.48788606],\n",
       "       [1.        , 0.22685145, 0.82365438, 0.47275191, 0.45711958],\n",
       "       ...,\n",
       "       [1.        , 0.98514494, 0.44507334, 0.12193146, 0.464396  ],\n",
       "       [1.        , 0.22066212, 0.27707334, 0.84701671, 0.12011312],\n",
       "       [1.        , 0.61329717, 0.27993699, 0.50973812, 0.34275199]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_matrix=np.c_[x_0,x_1,x_2,x_3,x_4]\n",
    "X_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeaf7f9",
   "metadata": {},
   "source": [
    "grad_of_rss function will retrun the gradient at a given data point. will also work with a mini batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0ec5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_of_rss(x_vals,Y_Predictions,Y_Reals):\n",
    "    delta=Y_Predictions-Y_Reals\n",
    "    delta_weights=np.copy(x_vals)\n",
    "    for i in range(len(delta)):\n",
    "        delta_weights[i]=delta[i]*x_vals[i]\n",
    "    #summing all columns into one cell and then taking the \n",
    "    #and finding the \"true\" values for the given input\n",
    "    #note that we taking the avg to also prevent overflow/underflow.\n",
    "    summations=delta_weights.sum(axis=0)/x_vals.shape[0]\n",
    "    return summations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e16e02",
   "metadata": {},
   "source": [
    "The Gradient Descent algorithm will get a step_size, the initial solution and the X values \n",
    "Along with the max_iter which is the maximum iterations of the algorithm.\n",
    "it will also get a function that will return the gradient of the RSS at a given data point.\n",
    "Weights will be initialized as 0.\n",
    "it will return the weights vector.\n",
    "\n",
    "The algorithm will run for max_iter iterations or until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8baf60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Descent(X,Y,Gradient_calculator=grad_of_rss,step_size=0.01,max_iter=1000,\n",
    "                     decay_rate=0,\n",
    "                     mini_batch=-1,\n",
    "                    gamma=0,v=0):\n",
    "    W=np.zeros(5)\n",
    "    X=np.copy(X_matrix)\n",
    "    Y=np.copy(Y_real)\n",
    "    #saving it just in case for exponential decay.\n",
    "    step_size_original=step_size\n",
    "    for i in range(max_iter):\n",
    "        Y_preds=X @ W\n",
    "        error=0.5*sum((Y_preds-Y_real)**2)\n",
    "        #print('the RSS error in iteration',i,' is',error)\n",
    "        #Update rule:\n",
    "        '''\n",
    "        if decay_rate has not been inputted, no exponential decay will happen,\n",
    "        otherwise stepsize will be as follows:\n",
    "        '''\n",
    "        if decay_rate!=0:\n",
    "            step_size=step_size_original*np.exp(-decay_rate * i)\n",
    "            \n",
    "        '''\n",
    "        if mini_batch >0 then Stochastic Gradient Descent will happen.\n",
    "        if mini_batch <=0 then Regular Gradient Descent will happen.\n",
    "        '''\n",
    "        if mini_batch>0:\n",
    "            random_sample=random.sample(range(len(X)),mini_batch)\n",
    "            # note that gradient calculator already does the averaging.\n",
    "            grad_at_pts=(Gradient_calculator(x_vals=X[random_sample],Y_Predictions=Y_preds[random_sample],Y_Reals=Y[random_sample]))\n",
    "            v=gamma*v+step_size*grad_at_pts\n",
    "            W=W-v\n",
    "        if mini_batch<=0:\n",
    "            W=W-step_size*(Gradient_calculator(x_vals=X,Y_Predictions=Y_preds,Y_Reals=Y))\n",
    "        ### Checking for convergence.\n",
    "        Y_pred_new= X @ W\n",
    "        new_error=0.5*sum((Y_pred_new-Y_real)**2)\n",
    "        change_in_error=np.abs(new_error-error)\n",
    "        if change_in_error<=0.1:\n",
    "            print('Gradient Converged at iteration',i,' with an error value of ',error)\n",
    "            return(W)\n",
    "    print('Reached Maximum Iterations',max_iter)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339de36",
   "metadata": {},
   "source": [
    "Testing Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab319445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Converged at iteration 141  with an error value of  4998.942604955297\n",
      "Gradient Descent Yields: W_1 = 1.03 || W_2 = -1.942 || W_3 = 2.997 || W_4 = -3.906\n"
     ]
    }
   ],
   "source": [
    "final_weights=Gradient_Descent(X=X_matrix,Y=Y_real,Gradient_calculator=grad_of_rss,step_size=0.396,max_iter=1000,)\n",
    "print(\"Gradient Descent Yields: W_1 =\",round(final_weights[1],3),\"|| W_2 =\",round(final_weights[2],3),\"|| W_3 =\",round(final_weights[3],3),\"|| W_4 =\",round(final_weights[4],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48541ec",
   "metadata": {},
   "source": [
    "We can see that Gradient Descent Yields pretty good Weights which are very close to the real weights of the Data Generation Process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b0949",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Exponential Decay:</span>\n",
    "Let's define exponential decay as:\n",
    "$$\\alpha_{epoch}=\\alpha_{first} * e^ {-DecayRate*Epoch}$$\n",
    "Where $$\\alpha_{epoch}, \\alpha_{first}$$ are the learning rate / step size.\n",
    "by adding a decay_rate parameter to the gradient descent function we can easily implement exponential decay into our algorithm.\n",
    "By setting decay_rate=0 as a default, we are back to having normal step_size calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ba9e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Converged at iteration 83  with an error value of  10958.847879299516\n",
      "Exponential Decay Yields: W_1 = 0.124 || W_2 = -0.726 || W_3 = 0.71 || W_4 = -1.303\n"
     ]
    }
   ],
   "source": [
    "final_weights=Gradient_Descent(X=X_matrix,Y=Y_real,Gradient_calculator=grad_of_rss,step_size=0.396,max_iter=1000,decay_rate=0.1)\n",
    "print(\"Exponential Decay Yields: W_1 =\",round(final_weights[1],3),\"|| W_2 =\",round(final_weights[2],3),\"|| W_3 =\",round(final_weights[3],3),\"|| W_4 =\",round(final_weights[4],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101b6f1",
   "metadata": {},
   "source": [
    "By having the same step_size as before but this time declaring a decay rate=0.1, we can see that the algorithm converged faster, but the error is worse. (premature stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8512e",
   "metadata": {},
   "source": [
    "#### <span style=\"color:purple\">Stochastic Gradient Descent:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233ee7b1",
   "metadata": {},
   "source": [
    "The main insight from stochastic gradient descent is that the gradient is an expectation. The expected value can be approximated using a small batch size. More specifically, we can sample a batch_size uniformly WITH REPLACEMENT from the train set.\n",
    "Let $$B^{sgd}$$ be the batch size and RSS the cost function.\n",
    "we can estimate the gradient as\n",
    "$$ \\hat g(x)= \\frac{1}{B^{sgd}} \\frac{d}{dw}\\sum_{i=1}^{B^{sgd}} L(x^{i},y^{i},w) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839b15a",
   "metadata": {},
   "source": [
    "adding mini_batch to the Gradient Descent function. By setting it to a positive integer, the algorithm will perform stochastic gradient descent, with the batch size which was inputted. By default, batch_size=-1 which will tell the algorithm to perform the regular Gradient Descent.\n",
    "\n",
    "Let's set mini_batch=64 and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dddacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Converged at iteration 152  with an error value of  5001.882569035912\n",
      "Stochastic Gradient Descent Yields: W_1 = 1.015 || W_2 = -1.992 || W_3 = 3.014 || W_4 = -3.916\n"
     ]
    }
   ],
   "source": [
    "final_weights=Gradient_Descent(X=X_matrix,Y=Y_real,Gradient_calculator=grad_of_rss,step_size=0.396,max_iter=20000,mini_batch=64)\n",
    "print(\"Stochastic Gradient Descent Yields: W_1 =\",round(final_weights[1],3),\"|| W_2 =\",round(final_weights[2],3),\"|| W_3 =\",round(final_weights[3],3),\"|| W_4 =\",round(final_weights[4],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566381e0",
   "metadata": {},
   "source": [
    "The algorithm converged after more iterations(when compared to vanilla GD), but these iterations were faster(in real life time) than the normal GD algorithm. The error value is similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3112b1",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\">Momentum</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e7c75",
   "metadata": {},
   "source": [
    "The velocity v accumulates the gradient elements. also the larger the gamma is relative to the step size, the more previous gradients affect the current direction of the momentum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75925b1d",
   "metadata": {},
   "source": [
    "In order to add the ability for momentum in the Gradient Descent function\n",
    "I will add two parameters\n",
    "v for initial velocity and gamma for the momentum parameter.\n",
    "both will be set to 0 by default. if both of them are non zero(along with mini_batch) the algorithm will perform Stochastic Gradient Descent with Momentum.\n",
    "Running Stochastic Gradient Descent with Momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e132c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Converged at iteration 442  with an error value of  5000.146305334952\n",
      "Momentum Gradient Descent Yields: W_1 = 1.002 || W_2 = -2.029 || W_3 = 2.989 || W_4 = -3.967\n"
     ]
    }
   ],
   "source": [
    "final_weights=Gradient_Descent(X=X_matrix,Y=Y_real,Gradient_calculator=grad_of_rss,step_size=0.396,max_iter=20000,mini_batch=64,gamma=0.222,v=1)\n",
    "print(\"Momentum Gradient Descent Yields: W_1 =\",round(final_weights[1],3),\"|| W_2 =\",round(final_weights[2],3),\"|| W_3 =\",round(final_weights[3],3),\"|| W_4 =\",round(final_weights[4],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704e496",
   "metadata": {},
   "source": [
    "$$\\tag*{$\\blacksquare$}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
